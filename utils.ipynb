{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978e776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh.plotting as bop\n",
    "import bokeh.models as bom\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
    "from scipy.stats import norm, gamma, lognorm\n",
    "from scipy.special import beta\n",
    "\n",
    "def split(X, y, n_train):\n",
    "    \"\"\"\n",
    "    Deterministic split of the data into training and test sets at n_train.\n",
    "    \"\"\"\n",
    "    X_train = X.iloc[:n_train, :]\n",
    "    X_test = X.iloc[n_train:, :]\n",
    "    y_train = y[:n_train]\n",
    "    y_test = y[n_train:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def crps_norm(y, loc, scale):\n",
    "    \"\"\"\n",
    "    Compute CRPS of a location-scale transformed normal distribution.\n",
    "    Translated from the R package `scoringRules`.\n",
    "    Source code: https://github.com/FK83/scoringRules/blob/master/R/scores_norm.R\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype=float)\n",
    "    y = y - loc\n",
    "    z = np.divide(y, scale, out=np.zeros_like(y), where=(~ np.isclose(y, 0) | ~ np.isclose(scale, 0)))\n",
    "    crps = scale * (z * (2 * norm.cdf(z) - 1) + 2 * norm.pdf(z) - 1 / np.sqrt(np.pi))\n",
    "    return crps\n",
    "\n",
    "def crps_gamma(y, shape, scale):\n",
    "    \"\"\"\n",
    "    Compute CRPS of a gamma distribution.\n",
    "    Translated from the R package `scoringRules`.\n",
    "    Source code: https://github.com/FK83/scoringRules/blob/master/R/scores_gamma.R\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype=float)\n",
    "    p1 = gamma.cdf(y, a=shape, scale=scale)\n",
    "    p2 = gamma.cdf(y, a=np.add(shape, 1), scale=scale)\n",
    "    crps = y * (2*p1 - 1) - scale * (shape * (2*p2 - 1) + 1 / beta(0.5, shape))\n",
    "    return crps\n",
    "\n",
    "def crps_lognorm(y, meanlog, sdlog):\n",
    "    \"\"\"\n",
    "    Compute CRPS of a lognormal distribution.\n",
    "    Translated from the R package `scoringRules`.\n",
    "    Source code: https://github.com/FK83/scoringRules/blob/master/R/scores_lnorm.R\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype=float)\n",
    "    c1 = y * (2 * lognorm.cdf(y, s=sdlog, scale=np.exp(meanlog)) - 1)\n",
    "    c2 = 2 * np.exp(np.add(meanlog, np.power(sdlog, 2) / 2))\n",
    "    c3 = lognorm.cdf(y, s=sdlog, scale=np.exp(np.add(meanlog, np.power(sdlog, 2)))) + norm.cdf(sdlog / np.sqrt(2)) - 1\n",
    "    crps = c1 - c2 * c3\n",
    "    return crps\n",
    "\n",
    "def evaluate_predictions(y, y_pred, categories, likelihood=\"gaussian\", **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model based on the predictions.\n",
    "    Specify the likelihood of the predictions with the keyword argument `likelihood`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: true values\n",
    "    y_pred: predicted values\n",
    "    categories: categories of the observations\n",
    "    likelihood: form of the response distribution, one of the following:\n",
    "        - \"gaussian\": normal distribution;\n",
    "        - \"gamma\": gamma distribution;\n",
    "        - \"lognorm\": lognormal distribution;\n",
    "        - \"loggamma\": loggamma distribution.\n",
    "    **kwargs: additional keyword arguments to specify the estimated parameters of the likelihood (to compute CRPS):\n",
    "        - \"gaussian\" requires `loc` and `scale`;\n",
    "        - \"gamma\" requires `shape` (or synonymously `gamma_shape`) and `gamma_scale`;\n",
    "        - \"lognorm\" requires `meanlog` and `sdlog`.\n",
    "        - \"loggamma\" requires `shape` (or synonymously `gamma_shape`) and `gamma_scale`.\n",
    "    \"\"\"\n",
    "    if likelihood == \"gaussian\":\n",
    "        loc = kwargs.get(\"loc\", y_pred)\n",
    "        scale = kwargs.get(\"scale\", np.sqrt(mean_squared_error(y, y_pred)))\n",
    "\n",
    "    # Take y and y_pred back to the original scale\n",
    "    if likelihood == \"lognorm\":\n",
    "        # Transform the observations back to the original scale\n",
    "        y = np.exp(y)\n",
    "        # Transform the predictions back to the original scale\n",
    "        meanlog = kwargs.get(\"meanlog\", y_pred)                                      # Infer the predictive parameters from predictions\n",
    "        sdlog = kwargs.get(\"sdlog\", np.sqrt(mean_squared_error(np.log(y), meanlog))) # Infer the predictive parameters from predictions\n",
    "        y_pred = np.exp(np.add(meanlog, np.power(sdlog, 2) / 2))\n",
    "\n",
    "    if likelihood in [\"gamma\", \"loggamma\"]:\n",
    "        gamma_shape = kwargs.get(\"gamma_shape\", kwargs.get(\"shape\", None))\n",
    "        gamma_scale = kwargs.get(\"gamma_scale\", None)\n",
    "        if gamma_shape is None:\n",
    "            # Estimate shape parameter of gamma distribution by Pearson's method\n",
    "            # Related discussion: https://stats.stackexchange.com/questions/367560/\n",
    "            # `statsmodels` ref: https://www.statsmodels.org/dev/_modules/statsmodels/genmod/generalized_linear_model.html#GLM.estimate_scale\n",
    "            resid = np.power(y - y_pred, 2)\n",
    "            var = np.power(y_pred, 2)\n",
    "            gamma_dispersion = np.sum(resid / var) / len(y)\n",
    "            gamma_shape = 1 / gamma_dispersion\n",
    "        if gamma_scale is None:\n",
    "            gamma_scale = y_pred / gamma_shape\n",
    "\n",
    "        if likelihood == \"loggamma\":\n",
    "            if any(gamma_scale >= 1):\n",
    "                raise ValueError(\"The scale parameter of the loggamma distribution must be < 1 otherwise expectation is infinite.\")\n",
    "            # Transform the observations back to the original scale\n",
    "            y = np.exp(y)\n",
    "            # Transform the predictions back to the original scale\n",
    "            y_pred = np.exp(y_pred)\n",
    "\n",
    "    scores = dict()\n",
    "\n",
    "    scores[\"MAE\"] = mean_absolute_error(y, y_pred)\n",
    "    scores[\"MedAE\"] = median_absolute_error(y, y_pred) # for a more robust estimate of the error\n",
    "    scores[\"MedPE\"] = np.median(np.divide(np.abs(np.subtract(y, y_pred)), y)) # median percentage error\n",
    "    scores[\"RMSE\"] = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    scores[\"R2\"] = r2_score(y, y_pred)\n",
    "\n",
    "    # RMSE of average prediction for each category\n",
    "    data = pd.concat([\n",
    "        pd.Series(categories, name=\"category\").reset_index(drop=True), \n",
    "        pd.Series(y, name=\"y\").reset_index(drop=True), \n",
    "        pd.Series(y_pred, name=\"y_pred\").reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "    gb = data.groupby(\"category\", as_index=False)\n",
    "    counts = gb.size()\n",
    "    avg_by_cat = gb[[\"y\", \"y_pred\"]].mean()\n",
    "    scores[\"RMSE_avg\"] = np.sqrt(mean_squared_error(avg_by_cat[\"y\"], avg_by_cat[\"y_pred\"]))\n",
    "\n",
    "    # Volume weighted RMSE of average prediction for each category\n",
    "    scores[\"RMSE_avg_weighted\"] = np.sqrt(\n",
    "        mean_squared_error(avg_by_cat[\"y\"], avg_by_cat[\"y_pred\"], sample_weight=counts[\"size\"]))\n",
    "\n",
    "    # CRPS to quantify accuracy of probabilistic predictions\n",
    "    if likelihood == \"gaussian\":\n",
    "        scores[\"CRPS\"] = crps_norm(y, loc, scale).mean()\n",
    "    elif likelihood == \"gamma\":\n",
    "        scores[\"CRPS\"] = crps_gamma(y, gamma_shape, gamma_scale).mean()\n",
    "    elif likelihood == \"lognorm\":\n",
    "        scores[\"CRPS\"] = crps_norm(np.log(y), meanlog, sdlog).mean()\n",
    "    elif likelihood == \"loggamma\":\n",
    "        scores[\"CRPS\"] = crps_gamma(np.log(y), gamma_shape, gamma_scale).mean()\n",
    "    \n",
    "    # Negative log-likelihood of probabilistic predictions\n",
    "    if likelihood == \"gaussian\":\n",
    "        scores[\"NLL\"] = -norm.logpdf(y, loc=loc, scale=scale).mean()\n",
    "    elif likelihood == \"gamma\":\n",
    "        scores[\"NLL\"] = -gamma.logpdf(y, a=gamma_shape, scale=gamma_scale).mean()\n",
    "    elif likelihood == \"lognorm\":\n",
    "        scores[\"NLL\"] = -np.log(norm.pdf(np.log(y), loc=meanlog, scale=sdlog) / y).mean()\n",
    "    elif likelihood == \"loggamma\":\n",
    "        scores[\"NLL\"] = -np.log(gamma.pdf(np.log(y), a=gamma_shape, scale=gamma_scale) / y).mean()\n",
    "\n",
    "    return scores\n",
    "\n",
    "def evaluate_model(regressor, X, y, categories, likelihood=\"gaussian\", **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset.\n",
    "    \"\"\"\n",
    "    y_pred = regressor.predict(X)\n",
    "    if (y_pred.ndim == 2) and (y_pred.shape[1] == 1):\n",
    "        y_pred = y_pred.flatten()\n",
    "    return evaluate_predictions(y, y_pred, categories, likelihood, **kwargs)\n",
    "\n",
    "# Colour scheme: https://coolors.co/palette/231942-5e548e-9f86c0-be95c4-e0b1cb\n",
    "def plot_from_model(model, X_train, y_train, X_test, y_test, log_scale=False, show=True):\n",
    "    \"\"\"\n",
    "    Plot the predictions vs. the true values for both the training and test sets.\n",
    "    \"\"\"\n",
    "    if (log_scale):\n",
    "        y_axis_type = \"log\"\n",
    "        x_axis_type = \"log\"\n",
    "    else:\n",
    "        y_axis_type = \"linear\"\n",
    "        x_axis_type = \"linear\"\n",
    "\n",
    "    X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "    y_train, y_test = pd.Series(y_train), pd.Series(y_test)\n",
    "    target = pd.concat([y_train, y_test])\n",
    "    left, right = target.min(), target.max()\n",
    "    span = right - left\n",
    "    bottom, top = target.min() - span * 0.1, target.max() + span * 0.1\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    if (y_pred_train.ndim == 2) and (y_pred_train.shape[1] == 1):\n",
    "        y_pred_train = y_pred_train.flatten()\n",
    "        y_pred_test = y_pred_test.flatten()\n",
    "    p1 = bop.figure(\n",
    "        title=\"Predictions vs ground truth\", \n",
    "        x_axis_label=\"Ground truth\", y_axis_label=\"Predictions\", \n",
    "        x_range=(left, right), y_range=(bottom, top),\n",
    "        x_axis_type=x_axis_type, y_axis_type=y_axis_type\n",
    "    )\n",
    "    p1.circle(y_train, y_pred_train, legend_label=\"In-sample\", color=\"#5e548e\")\n",
    "    p1.legend.location = \"top_left\"\n",
    "    p1.add_layout(bom.Slope(gradient=1, y_intercept=0, line_color=\"black\", line_width=2))\n",
    "    p1.output_backend = \"svg\"\n",
    "\n",
    "    p2 = bop.figure(\n",
    "        title=\"Predictions vs ground truth\", \n",
    "        x_axis_label=\"Ground truth\", y_axis_label=\"Predictions\", \n",
    "        x_range=(left, right), y_range=(bottom, top),\n",
    "        x_axis_type=x_axis_type, y_axis_type=y_axis_type\n",
    "    )\n",
    "    p2.circle(y_test, y_pred_test, legend_label=\"Out-of-sample\", color=\"#9f86c0\")\n",
    "    p2.legend.location = \"top_left\"\n",
    "    p2.add_layout(bom.Slope(gradient=1, y_intercept=0, line_color=\"black\", line_width=2))\n",
    "    p2.output_backend = \"svg\"\n",
    "\n",
    "    grid = gridplot([[p1, p2]], width=500, height=400)\n",
    "    if show:\n",
    "        bop.show(grid)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def plot_from_predictions(y_pred_train, y_train, y_pred_test, y_test, log_scale=False, show=True):\n",
    "    \"\"\"\n",
    "    Plot the predictions vs. the true values for both the training and test sets.\n",
    "    \"\"\"\n",
    "    if (log_scale):\n",
    "        y_axis_type = \"log\"\n",
    "        x_axis_type = \"log\"\n",
    "    else:\n",
    "        y_axis_type = \"linear\"\n",
    "        x_axis_type = \"linear\"\n",
    "\n",
    "    y_pred_train, y_pred_test = pd.Series(y_pred_train), pd.Series(y_pred_test)\n",
    "    y_train, y_test = pd.Series(y_train), pd.Series(y_test)\n",
    "    target = pd.concat([y_train, y_test])\n",
    "    left, right = target.min(), target.max()\n",
    "    span = right - left\n",
    "    bottom, top = target.min() - span * 0.1, target.max() + span * 0.1\n",
    "\n",
    "    p1 = bop.figure(\n",
    "        title=\"Predictions vs ground truth\", \n",
    "        x_axis_label=\"Ground truth\", y_axis_label=\"Predictions\", \n",
    "        x_range=(left, right), y_range=(bottom, top),\n",
    "        x_axis_type=x_axis_type, y_axis_type=y_axis_type\n",
    "    )\n",
    "    p1.circle(y_train, y_pred_train, legend_label=\"In-sample\", color=\"#5e548e\")\n",
    "    p1.legend.location = \"top_left\"\n",
    "    p1.add_layout(bom.Slope(gradient=1, y_intercept=0, line_color=\"black\", line_width=2))\n",
    "    p1.output_backend = \"svg\"\n",
    "\n",
    "    p2 = bop.figure(\n",
    "        title=\"Predictions vs ground truth\", \n",
    "        x_axis_label=\"Ground truth\", y_axis_label=\"Predictions\", \n",
    "        x_range=(left, right), y_range=(bottom, top),\n",
    "        x_axis_type=x_axis_type, y_axis_type=y_axis_type\n",
    "    )\n",
    "    p2.circle(y_test, y_pred_test, legend_label=\"Out-of-sample\", color=\"#9f86c0\")\n",
    "    p2.legend.location = \"top_left\"\n",
    "    p2.add_layout(bom.Slope(gradient=1, y_intercept=0, line_color=\"black\", line_width=2))\n",
    "    p2.output_backend = \"svg\"\n",
    "\n",
    "    grid = gridplot([[p1, p2]], width=500, height=400)\n",
    "    if show:\n",
    "        bop.show(grid)\n",
    "\n",
    "    return grid\n",
    "\n",
    "# Helper function to make tensorflow less verbose\n",
    "def set_tf_loglevel(level):\n",
    "    \"\"\"\n",
    "    Set the log level of TensorFlow.\n",
    "    Source: https://stackoverflow.com/a/57439591\n",
    "    \"\"\"\n",
    "    if level >= logging.FATAL:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    if level >= logging.ERROR:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    if level >= logging.WARNING:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    else:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    logging.getLogger('tensorflow').setLevel(level)\n",
    "\n",
    "# Helper function to plot loss curves of NN\n",
    "def plot_loss_curves(history, show=True):\n",
    "    \"\"\"\n",
    "    Plot the loss curves for the training and validation sets.\n",
    "    \"\"\"\n",
    "    p = bop.figure(width=1000, height=400, title=\"Loss curves\", x_axis_label=\"Epoch\", y_axis_label=\"Loss\")\n",
    "    # Omit the first data point by taking [1:] for a better plot\n",
    "    epochs = np.add(1, range(len(history[\"loss\"]))[1:])\n",
    "    p.line(epochs, history[\"loss\"][1:], legend_label=\"Training\", color=\"dodgerblue\")\n",
    "    p.line(epochs, history[\"val_loss\"][1:], legend_label=\"Validation\", color=\"lightskyblue\")\n",
    "    p.legend.location = \"top_right\"\n",
    "    if show:\n",
    "        bop.show(p)\n",
    "    \n",
    "    return p\n",
    "\n",
    "def embedding_preproc(X_train, X_test, cat_cols):\n",
    "    \"\"\"\n",
    "    Change data to list for processing with entity embeddings.\n",
    "    Code adapted from https://github.com/oegedijk/keras-embeddings/blob/master/build_embeddings.py\n",
    "    \"\"\"\n",
    "    input_list_train = []\n",
    "    input_list_test = []\n",
    "\n",
    "    for c in cat_cols:\n",
    "        input_list_train.append(X_train[c].values)\n",
    "        input_list_test.append(X_test[c].values)\n",
    "\n",
    "    return input_list_train, input_list_test\n",
    "\n",
    "def plot_ridgeline(categories, y, selected_cats, ax, title=None, ylabel=True, p_min=None, p_max=None):\n",
    "    \"\"\"\n",
    "    Plot a ridgeline plot of the data for selected categories.\n",
    "    Code adapted from https://scipython.com/blog/ridgeline-plots-of-monthly-uk-temperatures/\n",
    "    \"\"\"\n",
    "    num_cats = len(selected_cats)\n",
    "    if p_min is None or p_max is None:\n",
    "        p_min, p_max = np.min(y), np.max(y)\n",
    "    x_grid = np.linspace(p_min - 0.5, p_max + 0.5, 100)\n",
    "    cmap = mpl.cm.get_cmap(\"viridis\")\n",
    "    offset = 0.25\n",
    "    data = pd.DataFrame({\"y\": y, \"category\": categories})\n",
    "    y_mean = data.groupby(\"category\").mean()[\"y\"]\n",
    "    norm = mpl.colors.Normalize(vmin=y_mean.min(), vmax=y_mean.max())\n",
    "\n",
    "    ax.yaxis.set_tick_params(length=0, width=0)\n",
    "    ax.set_ylim(-0.01, num_cats * offset + 0.05)\n",
    "    if ylabel:\n",
    "        ax.set_yticks(np.arange(0, num_cats * offset, offset))\n",
    "        ax.set_yticklabels([f\"Category {i}\" for i in selected_cats])\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    for i, cat in enumerate(selected_cats):\n",
    "        c = cmap(norm(y_mean[cat]))\n",
    "        y_i = y[categories == cat]\n",
    "        dist = scipy.stats.gaussian_kde(y_i)\n",
    "        ax.plot(x_grid, dist(x_grid) / dist(x_grid).max() * 0.3 + offset * i, color=\"w\", zorder=num_cats + 1 - i)\n",
    "        ax.fill_between(x_grid, dist(x_grid) / dist(x_grid).max() * 0.3 + offset * i, offset * i, color=c, zorder=num_cats + 1 - i)\n",
    "        ax.axhline(offset * i, color=c, zorder=num_cats + 1 - i, linewidth=1)\n",
    "\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Export the predictions to a csv file (for plotting in R)\n",
    "def get_predictions(log_y, log_y_pred, dist, **kwargs):\n",
    "    if dist == \"lognormal\":\n",
    "        meanlog = log_y_pred\n",
    "        sdlog = kwargs.get(\"sdlog\", np.sqrt(mean_squared_error(log_y, log_y_pred)))\n",
    "        return pd.DataFrame({\n",
    "            \"log_y\": log_y,\n",
    "            \"meanlog\": meanlog,\n",
    "            \"sdlog\": sdlog,\n",
    "        })\n",
    "    elif dist == \"loggamma\":\n",
    "        gamma_shape = kwargs.get(\"gamma_shape\", None)\n",
    "        if gamma_shape is None:\n",
    "            # Estimate shape parameter of gamma distribution by Pearson's method\n",
    "            # Related discussion: https://stats.stackexchange.com/questions/367560/\n",
    "            # `statsmodels` ref: https://www.statsmodels.org/dev/_modules/statsmodels/genmod/generalized_linear_model.html#GLM.estimate_scale\n",
    "            resid = np.power(log_y - log_y_pred, 2)\n",
    "            var = np.power(log_y_pred, 2)\n",
    "            gamma_dispersion = np.sum(resid / var) / len(log_y)\n",
    "            gamma_shape = 1 / gamma_dispersion\n",
    "        gamma_scale = log_y_pred / gamma_shape\n",
    "        return pd.DataFrame({\n",
    "            \"log_y\": log_y,\n",
    "            \"gamma_shape\": gamma_shape,\n",
    "            \"gamma_scale\": gamma_scale,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a3c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
