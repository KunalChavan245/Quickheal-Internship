{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159622ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be253be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20b6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonNegative(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddd0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistParams(layers.Layer):\n",
    "    def __init__(self, inverse_link=\"identity\", phi_init=None, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inverse_link: str\n",
    "            Inverse link function. Default is \"identity\".\n",
    "            One of [\"exp\", \"identity\", \"log\", \"inverse\"].\n",
    "        phi_init: float or tf.keras.initializers.Initializer\n",
    "            Initial value of phi (scale parameter).\n",
    "        **kwargs: \n",
    "            Keyword arguments for the parent class, e.g. name.\n",
    "        \"\"\"\n",
    "        super(DistParams, self).__init__(**kwargs)\n",
    "\n",
    "        # Validate inputs\n",
    "        inverse_link_options = [\"exp\", \"identity\", \"log\", \"inverse\"]\n",
    "        if inverse_link not in inverse_link_options:\n",
    "            raise ValueError(f\"Expected {inverse_link!r} to be one of {inverse_link_options!r}\")\n",
    "        \n",
    "        # Define layer attributes\n",
    "        self.inverse_link = inverse_link\n",
    "        if self.inverse_link == \"exp\":\n",
    "            self.inverse_link_fn = tf.math.exp\n",
    "        elif self.inverse_link == \"identity\":\n",
    "            self.inverse_link_fn = tf.identity\n",
    "        elif self.inverse_link == \"log\":\n",
    "            self.inverse_link_fn = tf.math.log\n",
    "        elif self.inverse_link == \"inverse\":\n",
    "            self.inverse_link_fn = tf.math.reciprocal\n",
    "        \n",
    "        # Create layer weights that do not depend on input shapes\n",
    "        # Add phi (dispersion parameter)\n",
    "        self.phi_init = phi_init\n",
    "        if isinstance(phi_init, float) or isinstance(phi_init, int):\n",
    "            phi_init = tf.constant_initializer(phi_init)\n",
    "        self.phi = self.add_weight(\n",
    "            name=\"phi\",\n",
    "            shape=(1,),\n",
    "            initializer=phi_init if phi_init is not None else \"ones\",\n",
    "            constraint=NonNegative(),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, eta):\n",
    "        \"\"\"\n",
    "        Apply the inverse link function to f(X) + Zu, to obtain the conditional mean of y|u.\n",
    "        Input: eta = f(X) + Zu.\n",
    "        Output: loc_param = g^-1(eta), scale_param.\n",
    "        \"\"\"\n",
    "        loc_param = self.inverse_link_fn(eta)\n",
    "        scale_param = tf.expand_dims(tf.repeat(self.phi, tf.shape(eta)[0]), axis=1)\n",
    "        return tf.concat([loc_param, scale_param], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"inverse_link\": self.inverse_link,\n",
    "            \"phi_init\": self.phi_init,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def build_glmmnet(cardinality, num_vars, final_layer_likelihood, train_size, random_state=42, regularizer=False, is_prior_trainable=False):\n",
    "    \"\"\"\n",
    "    Build a GLMMNet model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cardinality: int, number of unique values in the high-cardinality variable of interest\n",
    "    num_vars: list of numerical variables\n",
    "    final_layer_likelihood: str, likelihood of the final layer, \"gaussian\" or \"gamma\"\n",
    "    train_size: int, number of training samples (used in kl_weight)\n",
    "    random_state: int, random seed\n",
    "    regularizer: bool, whether to use l2 regulariser, default is False\n",
    "    is_prior_trainable: bool, whether to train the prior parameters, default is False\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    # Construct a standard FFNN for the fixed effects\n",
    "    num_inputs = layers.Input(shape=(len(num_vars),), name=\"numeric_inputs\")\n",
    "    hidden_units = [64, 32, 16]\n",
    "    hidden_activation = \"relu\"\n",
    "    x = num_inputs\n",
    "    for hidden_layer in range(len(hidden_units)):\n",
    "        units = hidden_units[hidden_layer]\n",
    "        x = layers.Dense(\n",
    "            units=units, activation=hidden_activation, \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01) if regularizer else None,\n",
    "            name=f\"hidden_{hidden_layer + 1}\")(x)\n",
    "    f_X = layers.Dense(units=1, activation=\"linear\", name=\"f_X\")(x)\n",
    "\n",
    "    # Deal with categorical inputs (random effects)\n",
    "    cat_inputs = layers.Input(shape=(cardinality,), name=f\"category_OHE_inputs\")\n",
    "    # Construct the random effects, by variational inference\n",
    "    # Code adapted from https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression\n",
    "    # Specify the surrogate posterior over the random effects\n",
    "    def posterior_u(kernel_size, bias_size=0, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        return tf.keras.Sequential([\n",
    "            tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=\"random_normal\"),\n",
    "            tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "                tfd.Normal(loc=t[..., :n], scale=0.01 * tf.nn.softplus(t[..., n:])),\n",
    "                reinterpreted_batch_ndims=1)),\n",
    "        ])\n",
    "    # Specify the prior over the random effects\n",
    "    def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        c = np.log(np.expm1(0.1)) # Inverse of softplus()\n",
    "        return tf.keras.Sequential([\n",
    "            tfp.layers.VariableLayer(1, dtype=dtype, initializer=\"random_normal\"),\n",
    "            tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "                tfd.Normal(loc=tf.zeros(n), scale=tf.nn.softplus(c + t)),\n",
    "                reinterpreted_batch_ndims=1)),\n",
    "        ])\n",
    "    def prior_fixed(kernel_size, bias_size=0, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        return lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=tf.zeros(n), scale=0.1 * tf.ones(n)), \n",
    "            reinterpreted_batch_ndims=1\n",
    "        )\n",
    "    RE = tfp.layers.DenseVariational(\n",
    "        units=1, \n",
    "        make_posterior_fn=posterior_u,\n",
    "        make_prior_fn=prior_trainable if is_prior_trainable else prior_fixed,\n",
    "        kl_weight=1 / train_size,\n",
    "        use_bias=False,\n",
    "        activity_regularizer=tf.keras.regularizers.l2(0.01) if regularizer else None,\n",
    "        name=\"RE\",)(cat_inputs)\n",
    "\n",
    "    # Add the RE to the f(X) output\n",
    "    eta = layers.Add(name=\"f_X_plus_RE\")([f_X, RE])\n",
    "\n",
    "    # Build the final layer\n",
    "    if final_layer_likelihood == \"gaussian\":\n",
    "        # Compute the distributional parameters\n",
    "        dist_params = DistParams(inverse_link=\"identity\", phi_init=0.1, name=\"dist_params\",)(eta)\n",
    "        # Construct the distribution output\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(loc=t[..., :1], scale=t[..., 1:]), \n",
    "            name=\"distribution\",\n",
    "        )(dist_params)\n",
    "    elif final_layer_likelihood == \"gamma\":\n",
    "        # Compute the distributional parameters\n",
    "        dist_params = DistParams(\n",
    "            inverse_link=\"exp\",\n",
    "            phi_init=0.1,\n",
    "            name=\"dist_params\",)(eta)\n",
    "        # Construct the distribution output\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(                    # https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Gamma\n",
    "                concentration=1 / t[..., 1:],       # concentration = shape = 1 / dispersion\n",
    "                rate=(1 / t[..., 1:]) / t[..., :1], # rate = shape / location\n",
    "            ), name=\"distribution\")(dist_params)\n",
    "    \n",
    "    glmmnet = models.Model(inputs=[num_inputs, cat_inputs], outputs=output)\n",
    "\n",
    "    def NLL(y_true, y_dist_pred):\n",
    "        return -y_dist_pred.log_prob(y_true)\n",
    "    glmmnet.compile(optimizer=\"adam\", loss=NLL)\n",
    "\n",
    "    return glmmnet\n",
    "\n",
    "def predict_glmmnet(glmmnet, data, hicard_var, n_prediction_samples = 100):\n",
    "    \"\"\"\n",
    "    Predict the response variable for a given dataset from a fitted glmmnet model.\n",
    "    To make predictions, we call the model multiple times and average the results.\n",
    "    The randomness comes from that the RE are sampled from the posterior (in the DenseVariational layer).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glmmnet: a fitted glmmnet model.\n",
    "    data: a pandas dataframe with the same columns as the training data.\n",
    "    hicard_var: the name of the high-cardinality variable.\n",
    "    n_prediction_samples: the number of samples to average over when making predictions.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros((data.shape[0], n_prediction_samples))\n",
    "    hicard_columns = data.columns[data.columns.str.startswith(hicard_var)]\n",
    "    for i in tqdm.tqdm(range(n_prediction_samples)):\n",
    "        y_pred[:, i] = glmmnet((\n",
    "            tf.convert_to_tensor(data.drop(hicard_columns, axis=1)), \n",
    "            tf.convert_to_tensor(data[hicard_columns])), training=False).mean().numpy().flatten()\n",
    "    y_pred = y_pred.mean(axis=1)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def build_baseline_nn(X_train, objective=\"mse\", print_embeddings=False, random_state=42, cat_vars=[], num_vars=[]):\n",
    "    \"\"\"\n",
    "    Build a baseline neural network model with embeddings.\n",
    "    Code adapted from https://github.com/oegedijk/keras-embeddings/blob/72c1cfa29b1c57b5a14c24781f9dc713becb68ec/build_embeddings.py#L38\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(random_state)\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    for col in cat_vars:\n",
    "        # Estimate cardinality on the training set\n",
    "        cardinality = int(np.ceil(X_train[col].nunique()))\n",
    "        # Set the embedding dimension\n",
    "        embedding_dim = int(max(cardinality ** (1/4), 2))\n",
    "        if print_embeddings:\n",
    "            print(f'[{col}] cardinality: {cardinality} and embedding dim: {embedding_dim}')\n",
    "        \n",
    "        # Construct the embedding layer\n",
    "        col_inputs = layers.Input(shape=(1, ), name=col+\"_input\")\n",
    "        embedding = layers.Embedding(input_dim=cardinality, output_dim=embedding_dim, input_length=1, name=col+\"_embed\")(col_inputs)\n",
    "        # Use SpatialDropout to prevent overfitting\n",
    "        # See: https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
    "        embedding = layers.SpatialDropout1D(0.2, name=col+\"dropout\")(embedding)\n",
    "        # Flatten out the embeddings\n",
    "        embedding = layers.Reshape(target_shape=(embedding_dim,), name=col)(embedding)\n",
    "        # Add the input shape to inputs\n",
    "        inputs.append(col_inputs)\n",
    "        # Add the embeddings to the embeddings layer\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Add numeric inputs\n",
    "    num_inputs = layers.Input(shape=(len(num_vars),), name=\"numeric_inputs\")\n",
    "    inputs.append(num_inputs)\n",
    "\n",
    "    # Paste all the inputs together\n",
    "    x = layers.Concatenate(name=\"combined_inputs\")(embeddings + [num_inputs])\n",
    "\n",
    "    # Add some general NN layers\n",
    "    hidden_units = [64, 32, 16]\n",
    "    hidden_activation = \"relu\"\n",
    "    output_activation = \"linear\"\n",
    "    for hidden_layer in range(len(hidden_units)):\n",
    "        units = hidden_units[hidden_layer]\n",
    "        x = layers.Dense(units=units, activation=hidden_activation, name=f\"hidden_{hidden_layer + 1}\")(x)\n",
    "    output = layers.Dense(units=1, activation=output_activation, name=\"output\")(x)\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=objective, metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a01b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
