{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890a77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import category_encoders as ce   # https://contrib.scikit-learn.org/category_encoders/\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import make_friedman1, make_friedman3\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from category_encoders.wrapper import NestedCVWrapper\n",
    "from pathlib import Path\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "sys.path.append(\"../\")\n",
    "from glmmnet import build_glmmnet, predict_glmmnet, build_baseline_nn\n",
    "from utils import split, evaluate_model, evaluate_predictions, embedding_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8c7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim4(n_samples=100, noise=0.0, random_state=None):\n",
    "    \"\"\"Generate synthetic regression data.\n",
    "    Inputs `X` are 8 independent features uniformly distributed on the\n",
    "    intervals::\n",
    "        0 <= X[:, 0] <= 1,\n",
    "        0 <= X[:, 1] <= 1,\n",
    "        0 <= X[:, 2] <= 1,\n",
    "        0 <= X[:, 3] <= pi,\n",
    "        0 <= X[:, 4] <= 1,\n",
    "        0 <= X[:, 5] <= 100 * pi,\n",
    "        0 <= X[:, 6] <= 1/2,\n",
    "        0 <= X[:, 7] <= 1/2.\n",
    "    The output `y` is created according to the formula::\n",
    "        y(X) = sinh(X[:, 0] + X[:, 1]) + arccos(tanh(X[:, 2] + X[:, 3] \\\n",
    "            + X[:, 4])) + cos(X[:, 3] + X[:, 5]) + sec(X[:, 6] + X[:, 7]) \\\n",
    "            + noise * N(0, 1).\n",
    "    Adapted from http://www-scf.usc.edu/~tsangm/papers/nid_slides.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=100\n",
    "        The number of samples.\n",
    "    noise : float, default=0.0\n",
    "        The standard deviation of the gaussian noise applied to the output.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset noise. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (n_samples, 8)\n",
    "        The input samples.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The output values.\n",
    "    \"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "\n",
    "    X = rng.uniform(size=(n_samples, 8))\n",
    "    X[:, 3] *= np.pi\n",
    "    X[:, 5] *= 100 * np.pi\n",
    "    X[:, 6] *= 1/2\n",
    "    X[:, 7] *= 1/2\n",
    "    y = (np.sinh(X[:, 0] + X[:, 1]) + np.arccos(np.tanh(X[:, 2] + X[:, 3] + X[:, 4])) +\n",
    "         np.cos(X[:, 3] + X[:, 5]) + 1 / np.cos(X[:, 6] + X[:, 7])) + noise * rng.standard_normal(size=(n_samples))\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim5(n_samples=100, noise=0.0, random_state=None):\n",
    "    \"\"\"Generate synthetic regression data.\n",
    "    Inputs `X` are 8 independent features uniformly distributed on the\n",
    "    intervals::\n",
    "        0 <= X[:, 0] <= 1,\n",
    "        0 <= X[:, 1] <= 1,\n",
    "        0 <= X[:, 2] <= 4,\n",
    "        0 <= X[:, 3] <= 1/4,\n",
    "        0 <= X[:, 4] <= 2,\n",
    "        0 <= X[:, 5] <= 1/2,\n",
    "        0 <= X[:, 6] <= 1,\n",
    "        0 <= X[:, 7] <= 1.\n",
    "    The output `y` is created according to the formula::\n",
    "        y(X) = exp(abs(X[:, 0] - X[:, 1])) + abs(X[:, 2] * X[:, 3]) \\\n",
    "            - [X:, 3] ** [X:, 4] + log(X[:, 4] ** 2 + X[:, 5] ** 2) \\\n",
    "            + X[:, 6] + 1 / (1 + X[:, 7] ** 2) + noise * N(0, 1).\n",
    "    Adapted from http://www-scf.usc.edu/~tsangm/papers/nid_slides.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=100\n",
    "        The number of samples.\n",
    "    noise : float, default=0.0\n",
    "        The standard deviation of the gaussian noise applied to the output.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset noise. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (n_samples, 8)\n",
    "        The input samples.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The output values.\n",
    "    \"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "\n",
    "    X = rng.uniform(size=(n_samples, 8))\n",
    "    X[:, 2] *= 4\n",
    "    X[:, 3] *= 1/4\n",
    "    X[:, 4] *= 2\n",
    "    X[:, 5] *= 1/2\n",
    "    y = (np.exp(np.abs(X[:, 0] - X[:, 1])) + np.abs(X[:, 2] * X[:, 3]) -\n",
    "            X[:, 3] ** X[:, 4] + np.log(X[:, 4] ** 2 + X[:, 5] ** 2) +\n",
    "            X[:, 6] + 1 / (1 + X[:, 7] ** 2)) + noise * rng.standard_normal(size=(n_samples))\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9e99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim(\n",
    "        X, f_X, n_categories, signal_to_noise=np.array([4, 2, 1]),\n",
    "        y_dist=\"gaussian\", inverse_link=None, cat_dist=\"balanced\", random_state=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data from mixed effects models:\n",
    "        `g(mu) = f(X) + Z*u`,\n",
    "    where `f(X)` is a nonlinear function of `X` (feature matrix), `Z` is \n",
    "    a matrix of random effects variables, `u` is a vector of random effects,\n",
    "    and `g(mu)` is a nonlinear function of `mu` (mean response).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : feature matrix of fixed effects, pd.DataFrame of shape (n_samples, n_features).\n",
    "    f_X : nonlinear deterministic function of `X`, ndarray of shape (n_samples, ).\n",
    "    n_categories : int, number of categories/groups.\n",
    "    signal_to_noise : ndarray of shape (3, ), default=np.array([4, 2, 1])\n",
    "        The relative ratio of signal from fixed effects, signal from random effects, \n",
    "        and noise in the response. It will be normalized to sum to 1.\n",
    "    y_dist : str, default=\"gaussian\"\n",
    "        The distribution of the response variable, \"gaussian\", \"gamma\", or \"lognormal\".\n",
    "    inverse_link : callable, inverse of link function.\n",
    "        If None, the exp function is used when y_dist=\"gamma\", and the identity function\n",
    "        is used otherwise.\n",
    "    cat_dist : str, default=\"balanced\"\n",
    "        \"balanced\" or \"skewed\" distributions for the allocation of categories.\n",
    "        \"balanced\" allocates approx equal number of observations to each category.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset noise. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame of shape (n_samples, n_features + 1).\n",
    "        The input samples, including a column of category labels from 0 to n_categories-1.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The output values.\n",
    "    truth : ndarray of shape (n_samples,)\n",
    "        The true mean response values (unobservable in practice).\n",
    "    Zu : ndarray of shape (n_samples,)\n",
    "        The raw random effects.\n",
    "    \"\"\"\n",
    "    rng = check_random_state(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    X = pd.DataFrame(X, columns=[\"X\" + str(i) for i in range(1, n_features + 1)])\n",
    "    if (cat_dist == \"balanced\"):\n",
    "        X = X.assign(\n",
    "            # Generate a random number from 0 to (n_categories - 1)\n",
    "            category = rng.randint(low=0, high=n_categories, size=n_samples).astype(object)\n",
    "        )\n",
    "    elif (cat_dist == \"skewed\"):\n",
    "        X = X.assign(\n",
    "            category = np.floor(rng.beta(a=2, b=3, size=n_samples) * n_categories).astype(int).astype(object)\n",
    "        )\n",
    "    \n",
    "    # Generate random effects\n",
    "    signal_to_noise = signal_to_noise / sum(signal_to_noise)\n",
    "    signal_FE, signal_RE, noise = tuple(signal_to_noise)\n",
    "    u = rng.standard_normal(size=n_categories) * signal_RE\n",
    "    Zu = u[X.category.astype(int)]\n",
    "\n",
    "    # Scale the fixed effects so that the mean of f(X) = signal_FE\n",
    "    f_X = f_X / f_X.mean() * signal_FE\n",
    "\n",
    "    # Generate response variable\n",
    "    if inverse_link is None:\n",
    "        # By default, use log link for gamma and identity link for lognormal or gaussian\n",
    "        inverse_link = np.exp if y_dist == \"gamma\" else (lambda x: x)\n",
    "    truth = inverse_link(f_X + Zu)\n",
    "    if (y_dist == \"gaussian\"):\n",
    "        y = rng.normal(loc=truth, scale=noise, size=n_samples)\n",
    "    elif (y_dist == \"gamma\"):\n",
    "        # We need:\n",
    "        # 1. gamma_mean = truth, i.e. gamma_shape * gamma_scale = truth\n",
    "        # 2. gamma_variance = (noise ** 2) * (truth ** 2), where gamma_variance = gamma_mean ** 2 / gamma_shape\n",
    "        gamma_shape = 1 / (noise ** 2)\n",
    "        gamma_scale = (noise ** 2) * truth\n",
    "        y = rng.gamma(shape=gamma_shape, scale=gamma_scale, size=n_samples)\n",
    "    elif (y_dist == \"lognormal\"):\n",
    "        # 1. ln_mean = truth\n",
    "        # 2. ln_variance = noise ** 2\n",
    "        ln_sigma = np.sqrt(np.log(1 + (noise ** 2) / (truth ** 2)))\n",
    "        ln_mean = np.log(truth) - ln_sigma ** 2 / 2\n",
    "        y = rng.lognormal(mean=ln_mean, sigma=ln_sigma, size=n_samples)\n",
    "    \n",
    "    return X, y, truth, Zu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b15c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "        exp_id, n_train=5000, n_test=2500, n_categories=100, f_structure=\"friedman1\",\n",
    "        signal_to_noise=np.array([4, 2, 1]), y_dist=\"gaussian\", inverse_link=lambda x: x, \n",
    "        cat_dist=\"balanced\", random_state=None, overwrite=\"auto\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data from mixed effects models:\n",
    "        `g(mu) = f(X) + Z*u`,\n",
    "    where `f(X)` is a nonlinear function of `X` given by `f_structure`, `Z` is \n",
    "    a matrix of random effects variables, `u` is a vector of random effects,\n",
    "    and `g(mu)` is a nonlinear function of `mu` (mean response).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_id : int, experiment id.\n",
    "    n_train : int, number of training samples.\n",
    "    n_test : int, number of test samples.\n",
    "    n_categories : int, number of categories.\n",
    "    f_structure : str, structure of f(X), choose from \"friedman1\" (default), \"friedman3\".\n",
    "    signal_to_noise : ndarray of shape (3, ), default=np.array([4, 2, 1])\n",
    "        The relative ratio of signal from fixed effects, signal from random effects, \n",
    "        and noise in the response. It will be normalized to sum to 1.\n",
    "    y_dist : str, default=\"gaussian\"\n",
    "        The distribution of the response variable, \"gaussian\", \"gamma\", or \"lognormal\".\n",
    "    inverse_link : callable, inverse of link function.\n",
    "        If None, the exp function is used when y_dist=\"gamma\", and the identity function\n",
    "        is used otherwise.\n",
    "    cat_dist : str, default=\"balanced\"\n",
    "        \"balanced\" or \"skewed\" distributions for the allocation of categories.\n",
    "        \"balanced\" allocates approx equal number of observations to each category.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset noise. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "    overwrite : bool, default=\"auto\"\n",
    "        Whether to overwrite existing data. If \"auto\", it will be set to True if\n",
    "        the data does not exist, and False otherwise.\n",
    "    suffix : str, default=\"\"\n",
    "        Suffix to append to the file name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of dataframes, with keys \"train\", \"test\", \"Zu\".\n",
    "    \"\"\"\n",
    "    n = n_train + n_test\n",
    "    if (f_structure == \"friedman1\"):\n",
    "        # Out of the n_features features, only 5 are actually used to compute y.\n",
    "        # The remaining features are independent of y.\n",
    "        X, f_X = make_friedman1(n_samples=n, n_features=10, noise=0.0, random_state=random_state)\n",
    "    elif (f_structure == \"friedman3\"):\n",
    "        X, f_X = make_friedman3(n_samples=n, noise=0.0, random_state=random_state)\n",
    "\n",
    "    # Simulate random effects and therefore the response variable y\n",
    "    X, y, truth, Zu = make_sim(X, f_X, n_categories, signal_to_noise, y_dist, inverse_link, cat_dist, random_state)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = split(X, y, n_train)\n",
    "\n",
    "    # Conditional mean true values (unobservable in practice)\n",
    "    y_true_train = truth[:n_train]\n",
    "    y_true_test = truth[n_train:]\n",
    "\n",
    "    # Combine X, y, and truth into a dataframe and save to files\n",
    "    train_data = pd.concat([\n",
    "        X_train.reset_index(drop=True), \n",
    "        pd.DataFrame(y_train, columns=[\"y\"]), \n",
    "        pd.DataFrame(y_true_train, columns=[\"y_true\"])], axis=1)\n",
    "    test_data = pd.concat([\n",
    "        X_test.reset_index(drop=True),\n",
    "        pd.DataFrame(y_test, columns=[\"y\"]),\n",
    "        pd.DataFrame(y_true_test, columns=[\"y_true\"])], axis=1)\n",
    "\n",
    "    path = f\"data/experiment_{exp_id}\"\n",
    "    if (overwrite == \"auto\"):\n",
    "        overwrite = not Path(f\"{path}/train_data{suffix}.csv\").exists()\n",
    "    if (overwrite):\n",
    "        train_data.to_csv(f\"{path}/train_data{suffix}.csv\", index=False)\n",
    "        test_data.to_csv(f\"{path}/test_data{suffix}.csv\", index=False)\n",
    "    \n",
    "    return {\n",
    "        \"train\": (X_train, y_train, y_true_train),\n",
    "        \"test\": (X_test, y_test, y_true_test),\n",
    "        \"Zu\": Zu,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2185d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to run simulation experiments similar to run_sim_experiment in the demo notebook\n",
    "# This is a lighter version that only computes the test (and no training) performance\n",
    "# Requires saved experiment results for GLMM and GPBoost (see R script)\n",
    "def run_sim_experiment_light(exp_id, sim_data, y_dist=\"gaussian\", random_state=42, suffix=\"\"):\n",
    "\n",
    "    # Unpack data\n",
    "    X_train, y_train, y_true_train = sim_data[\"train\"]\n",
    "    X_test, y_test, y_true_test = sim_data[\"test\"]\n",
    "    Zu = sim_data[\"Zu\"]\n",
    "\n",
    "    # Variable names\n",
    "    hicard_var = \"category\"\n",
    "    x_num = [col for col in X_train.columns if col not in hicard_var]\n",
    "    colnames = [hicard_var] + x_num\n",
    "\n",
    "    # Convert numeric to string so they can be recognised by ce.OrdinalEncoder\n",
    "    X_train[hicard_var] = X_train[hicard_var].astype(\"str\")\n",
    "    X_test[hicard_var] = X_test[hicard_var].astype(\"str\")\n",
    "\n",
    "    # Initialise\n",
    "    test_scores = dict()\n",
    "\n",
    "    # GLM\n",
    "    # -------------------------------------------------------------------------\n",
    "    ignore_cat_encoder = make_column_transformer(\n",
    "        (\"drop\", [hicard_var]),\n",
    "        (MinMaxScaler(feature_range=(0, 1)), x_num), # not required for GLMs, applied here for consistency\n",
    "    )\n",
    "    X_train_ic = ignore_cat_encoder.fit_transform(X_train)\n",
    "    X_test_ic = ignore_cat_encoder.transform(X_test)\n",
    "    if y_dist == \"gaussian\":\n",
    "        GLM_ignore_cat = LinearRegression()\n",
    "        GLM_ignore_cat.fit(X_train_ic, y_train)\n",
    "        gamma_shape = None # placeholder, will not be read when y_dist == \"gaussian\"\n",
    "    elif y_dist == \"gamma\":\n",
    "        GLM_ignore_cat = sm.GLM(y_train, X_train_ic, family=sm.families.Gamma(sm.families.links.log())).fit()\n",
    "        gamma_shape = 1 / GLM_ignore_cat.scale\n",
    "    test_scores[\"GLM_ignore_cat\"] = evaluate_model(\n",
    "        GLM_ignore_cat, X_test_ic, y_test, categories=X_test[hicard_var], \n",
    "        likelihood=y_dist, gamma_shape=gamma_shape)\n",
    "\n",
    "    one_hot_encoder = make_column_transformer(\n",
    "        (OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False), [hicard_var]),\n",
    "        (MinMaxScaler(feature_range=(0, 1)), x_num), # not required for GLMs, applied here for consistency\n",
    "    )\n",
    "    X_train_ohe = one_hot_encoder.fit_transform(X_train)\n",
    "    X_test_ohe = one_hot_encoder.transform(X_test)\n",
    "    if y_dist == \"gaussian\":\n",
    "        GLM_one_hot = LinearRegression()\n",
    "        GLM_one_hot.fit(X_train_ohe, y_train)\n",
    "        gamma_shape = None # placeholder, will not be read when y_dist == \"gaussian\"\n",
    "    elif y_dist == \"gamma\":\n",
    "        GLM_one_hot = sm.GLM(y_train, X_train_ohe, family=sm.families.Gamma(sm.families.links.log())).fit()\n",
    "        gamma_shape = 1 / GLM_one_hot.scale\n",
    "    test_scores[\"GLM_one_hot\"] = evaluate_model(\n",
    "        GLM_one_hot, X_test_ohe, y_test, categories=X_test[hicard_var], \n",
    "        likelihood=y_dist, gamma_shape=gamma_shape)\n",
    "\n",
    "    GLMM_encoder = make_column_transformer(\n",
    "        # https://contrib.scikit-learn.org/category_encoders/wrapper.html\n",
    "        (NestedCVWrapper(ce.glmm.GLMMEncoder(), cv=KFold(n_splits=5)), [hicard_var]),\n",
    "        (MinMaxScaler(feature_range=(0, 1)), x_num), # not required for GLMs, applied here for consistency\n",
    "    )\n",
    "    X_train_GLMM_enc = GLMM_encoder.fit_transform(X_train, y_train)\n",
    "    X_test_GLMM_enc = GLMM_encoder.transform(X_test)\n",
    "    if y_dist == \"gaussian\":\n",
    "        GLM_GLMM_enc = LinearRegression()\n",
    "        GLM_GLMM_enc.fit(X_train_GLMM_enc, y_train)\n",
    "        gamma_shape = None # placeholder, will not be read when y_dist == \"gaussian\"\n",
    "    elif y_dist == \"gamma\":\n",
    "        GLM_GLMM_enc = sm.GLM(y_train, X_train_GLMM_enc, family=sm.families.Gamma(sm.families.links.log())).fit()\n",
    "        gamma_shape = 1 / GLM_GLMM_enc.scale\n",
    "    test_scores[\"GLM_GLMM_enc\"] = evaluate_model(\n",
    "        GLM_GLMM_enc, X_test_GLMM_enc, y_test, categories=X_test[hicard_var], \n",
    "        likelihood=y_dist, gamma_shape=gamma_shape)\n",
    " # GBM\n",
    "    # -------------------------------------------------------------------------\n",
    "    GBM_ignore_cat = xgb.XGBRegressor(n_jobs=-1, learning_rate=0.1, max_depth=2, random_state=random_state)\n",
    "    GBM_ignore_cat.fit(X_train_ic, y_train)\n",
    "    test_scores[\"GBM_ignore_cat\"] = evaluate_model(GBM_ignore_cat, X_test_ic, y_test, categories=X_test[hicard_var], likelihood=y_dist)\n",
    "\n",
    "    GBM_one_hot = xgb.XGBRegressor(n_jobs=-1, learning_rate=0.1, max_depth=2, random_state=random_state)\n",
    "    GBM_one_hot.fit(X_train_ohe, y_train)\n",
    "    test_scores[\"GBM_one_hot\"] = evaluate_model(GBM_one_hot, X_test_ohe, y_test, categories=X_test[hicard_var], likelihood=y_dist)\n",
    "\n",
    "    GBM_GLMM_enc = xgb.XGBRegressor(n_jobs=-1, learning_rate=0.1, max_depth=2, random_state=random_state)\n",
    "    GBM_GLMM_enc.fit(X_train_GLMM_enc, y_train)\n",
    "    test_scores[\"GBM_GLMM_enc\"] = evaluate_model(GBM_GLMM_enc, X_test_GLMM_enc, y_test, categories=X_test[hicard_var], likelihood=y_dist)\n",
    "     # NN with entity embeddings\n",
    "    # -------------------------------------------------------------------------\n",
    "    ct_nn = make_column_transformer(\n",
    "        (ce.ordinal.OrdinalEncoder(handle_unknown=\"value\"), [hicard_var]), # \"value\" encodes unknown categories as \"-1\"\n",
    "        (MinMaxScaler(feature_range=(0, 1)), x_num),\n",
    "    )\n",
    "    X_train_ct = pd.DataFrame(ct_nn.fit_transform(X_train), columns=colnames)\n",
    "    X_test_ct = pd.DataFrame(ct_nn.transform(X_test), columns=colnames)\n",
    "    X_embed_train, X_embed_test = embedding_preproc(X_train_ct, X_test_ct, [hicard_var])\n",
    "\n",
    "    # Define training parameters\n",
    "    epochs = 500\n",
    "    patience = 50\n",
    "    batch_size = 256\n",
    "    NN_ee = build_baseline_nn(X_train_ct, random_state=random_state, num_vars=x_num, cat_vars=[hicard_var])\n",
    "    es = EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\", verbose=2)\n",
    "\n",
    "    hist = NN_ee.fit(\n",
    "        (tuple(X_embed_train), X_train_ct[x_num]), y_train, validation_split=0.2, \n",
    "        epochs=epochs, callbacks=[es], batch_size=batch_size, verbose=0)\n",
    "    y_pred_test = NN_ee.predict((tuple(X_embed_test), X_test_ct[x_num])).flatten()\n",
    "    test_scores[\"NN_ee\"] = evaluate_predictions(y_test, y_pred_test, categories=X_test[hicard_var], likelihood=y_dist)\n",
    "\n",
    "    # Extract embeddings from the fitted neural network model\n",
    "    embeddings = NN_ee.get_layer(f\"{hicard_var}_embed\").get_weights()[0]\n",
    "    embeddings = pd.DataFrame(embeddings, columns=[\"embedding_\" + str(i) for i in range(embeddings.shape[1])])\n",
    "    oe = ct_nn.named_transformers_[\"ordinalencoder\"]\n",
    "    categories = np.array(oe.category_mapping[0][\"mapping\"])[:-1]\n",
    "    embeddings = pd.concat([pd.DataFrame(categories, columns=[hicard_var]), embeddings], axis=1)\n",
    "\n",
    "    # GBM with entity embeddings\n",
    "    # -------------------------------------------------------------------------\n",
    "    # .fillna(0) fills zeros for missing values in the embeddings (i.e. unknown categories)\n",
    "    X_train_ee = pd.merge(X_train_ct, embeddings, on=\"category\", how=\"left\").drop(\"category\", axis=1)\n",
    "    X_test_ee = pd.merge(X_test_ct, embeddings, on=\"category\", how=\"left\").drop(\"category\", axis=1).fillna(0)\n",
    "    GBM_ee = xgb.XGBRegressor(n_jobs=-1, learning_rate=0.1, max_depth=2, random_state=random_state)\n",
    "    GBM_ee.fit(X_train_ee, y_train)\n",
    "    test_scores[\"GBM_ee\"] = evaluate_model(GBM_ee, X_test_ee, y_test, categories=X_test[hicard_var], likelihood=y_dist)\n",
    "\n",
    "    # GLMM\n",
    "    # -------------------------------------------------------------------------\n",
    "    y_pred_test_brms = pd.read_csv(f\"models/experiment_{exp_id}/brms_pred_test{suffix}.csv\").values.reshape(-1)\n",
    "    sigma_brms = pd.read_csv(f\"models/experiment_{exp_id}/brms_sigma{suffix}.csv\").values.reshape(-1)\n",
    "    if y_dist == \"gaussian\":\n",
    "        custom_params_test = {\"loc\": y_pred_test_brms, \"scale\": sigma_brms}\n",
    "    elif y_dist == \"gamma\":\n",
    "        custom_params_test = {\"gamma_shape\": sigma_brms, \"gamma_scale\": y_pred_test_brms / sigma_brms}\n",
    "    test_scores[\"GLMM\"] = evaluate_predictions(\n",
    "        y_test, y_pred_test_brms, categories=X_test[hicard_var], likelihood=y_dist,\n",
    "        **custom_params_test\n",
    "    )\n",
    "    \n",
    "    # GPBoost\n",
    "    # -------------------------------------------------------------------------\n",
    "    y_pred_test_gpb = pd.read_csv(f\"models/experiment_{exp_id}/gpboost_pred_test{suffix}.csv\").values.reshape(-1)\n",
    "    scale_gpb = pd.read_csv(f\"models/experiment_{exp_id}/gpboost_scale{suffix}.csv\").values.reshape(-1)\n",
    "    test_scores[\"GPBoost\"] = evaluate_predictions(\n",
    "        y_test, y_pred_test_gpb, categories=X_test[hicard_var], likelihood=y_dist,\n",
    "        loc=y_pred_test_gpb, scale=np.sqrt(scale_gpb),\n",
    "    )\n",
    "\n",
    "    # GLMMNet (proposed model)\n",
    "    # -------------------------------------------------------------------------\n",
    "    set_seed(random_state)\n",
    "    one_hot = ce.OneHotEncoder(cols=[hicard_var], handle_unknown=\"value\")\n",
    "    X_train_ct_ohe = one_hot.fit_transform(X_train_ct)\n",
    "    X_test_ct_ohe = one_hot.transform(X_test_ct)\n",
    "    cardinality = len(one_hot.category_mapping[0][\"mapping\"].keys()) - 1\n",
    "\n",
    "    glmmnet = build_glmmnet(\n",
    "        cardinality=cardinality, num_vars=x_num, final_layer_likelihood=y_dist, \n",
    "        train_size=X_train.shape[0], random_state=random_state)\n",
    "    es = EarlyStopping(patience=50, restore_best_weights=True, monitor=\"val_loss\", verbose=2)\n",
    "    hist = glmmnet.fit(\n",
    "        (X_train_ct.drop([hicard_var], axis=1), X_train_ct_ohe.loc[:, X_train_ct_ohe.columns.str.startswith(hicard_var)]), y_train, \n",
    "        validation_split=0.2,\n",
    "        callbacks=[es],\n",
    "        # batch_size = 128 or 256 doesn't make a difference here, but 256 is faster\n",
    "        batch_size=256, epochs=500, verbose=False)\n",
    "\n",
    "    y_pred_test = predict_glmmnet(glmmnet, X_test_ct_ohe, hicard_var)\n",
    "    phi = glmmnet.get_layer(\"dist_params\").get_weights()\n",
    "    test_scores[\"GLMMNet\"] = evaluate_predictions(\n",
    "        y_test, y_pred_test, categories=X_test[hicard_var], likelihood=y_dist,\n",
    "        loc=y_pred_test, scale=phi[0][0], gamma_shape=1 / phi[0][0])\n",
    "\n",
    "    # GLMMNet with l2-regularizer\n",
    "    # -------------------------------------------------------------------------\n",
    "    glmmnet_l2 = build_glmmnet(\n",
    "        cardinality=cardinality, num_vars=x_num, final_layer_likelihood=y_dist, \n",
    "        train_size=X_train.shape[0], random_state=random_state, regularizer=True)\n",
    "    es = EarlyStopping(patience=50, restore_best_weights=True, monitor=\"val_loss\", verbose=2)\n",
    "    hist = glmmnet_l2.fit(\n",
    "        (X_train_ct.drop([hicard_var], axis=1), X_train_ct_ohe.loc[:, X_train_ct_ohe.columns.str.startswith(hicard_var)]), y_train, \n",
    "        validation_split=0.2,\n",
    "        callbacks=[es],\n",
    "        # batch_size = 128 or 256 doesn't make a difference here, but 256 is faster\n",
    "        batch_size=256, epochs=500, verbose=False)\n",
    "\n",
    "    y_pred_test = predict_glmmnet(glmmnet_l2, X_test_ct_ohe, hicard_var)\n",
    "    phi = glmmnet_l2.get_layer(\"dist_params\").get_weights()\n",
    "    test_scores[\"GLMMNet_l2\"] = evaluate_predictions(\n",
    "        y_test, y_pred_test, categories=X_test[hicard_var], likelihood=y_dist,\n",
    "        loc=y_pred_test, scale=phi[0][0], gamma_shape=1 / phi[0][0])\n",
    "    \n",
    "    return test_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20843c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim4: \n",
      "\n",
      "0.20506709735851436 6.523938859353687 2.698854347830454\n",
      "\n",
      "\n",
      "sim5: \n",
      "\n",
      "-5.766023769354329 6.673197333061368 2.5450551279197904\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    X, F = make_sim4(n_samples=10000)\n",
    "    print(\"sim4: \\n\")\n",
    "    # print(X[:10, :])\n",
    "    print(F.min(), F.max(), F.mean())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    X, F = make_sim5(n_samples=10000)\n",
    "    print(\"sim5: \\n\")\n",
    "    # print(X[:10, :])\n",
    "    print(F.min(), F.max(), F.mean())\n",
    "\n",
    "    X, F = make_friedman1(n_samples=1000, n_features=10, random_state=42)\n",
    "    stn = np.array([1, 1, 1e-3])\n",
    "    X, y, truth, Zu = make_sim(X, F, y_dist=\"lognormal\", inverse_link=np.exp, n_categories=100, signal_to_noise=stn, random_state=42)\n",
    "    # Check plot in an interactive environment\n",
    "    # When stn[2] == 0, the plot should be a straight line\n",
    "    _ = pd.DataFrame({\"truth\": truth, \"y\": y}).plot.scatter(x=\"truth\", y = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebec478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
